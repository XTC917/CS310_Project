{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56450427",
   "metadata": {},
   "source": [
    "# 简单二分+tfidf，中文模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8142d26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 人工文件: ['face2_zh_json/human/zh_unicode\\\\news-zh.json', 'face2_zh_json/human/zh_unicode\\\\webnovel.json', 'face2_zh_json/human/zh_unicode\\\\wiki-zh.json']\n",
      "✅ 千问文件: ['face2_zh_json/generated/zh_qwen2\\\\news-zh.qwen2-72b-base.json', 'face2_zh_json/generated/zh_qwen2\\\\webnovel.qwen2-72b-base.json', 'face2_zh_json/generated/zh_qwen2\\\\wiki-zh.qwen2-72b-base.json']\n"
     ]
    }
   ],
   "source": [
    "# 0. 导入库\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "# 1. 读取数据集路径\n",
    "human_path = \"face2_zh_json/human/zh_unicode\"\n",
    "llm_path = \"face2_zh_json/generated/zh_qwen2\"\n",
    "\n",
    "human_files = [os.path.join(human_path, f) for f in os.listdir(human_path)]\n",
    "llm_files = [os.path.join(llm_path, f) for f in os.listdir(llm_path)]\n",
    "\n",
    "print(\"✅ 人工文件:\", human_files[:3])\n",
    "print(\"✅ 千问文件:\", llm_files[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4436cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👀 人工数据样例：\n",
      "                                              output  label\n",
      "0  补贴后，变成了以积分、抽奖等形式为主的“暗补”。一组公开的数据显示，停补后的“滴滴打车”日均...      0\n",
      "1  培训、投融资等方面有着巨大的合作空间，为积极推动我省与丹麦的友好交流，促进双边经贸投资合作，...      0\n",
      "2                               环结你喜欢哪种呢？觉得不错，请点赞↓↓↓      0\n"
     ]
    }
   ],
   "source": [
    "# 2. 加载人工数据\n",
    "def load_human_data(file):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    df = pd.DataFrame(data)\n",
    "    df[\"label\"] = 0  # 人工标记为0\n",
    "    return df[[\"output\", \"label\"]]\n",
    "\n",
    "human_df = pd.concat([load_human_data(f) for f in human_files], ignore_index=True)\n",
    "print(\"👀 人工数据样例：\")\n",
    "print(human_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "468e948e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 千问生成数据样例：\n",
      "                                              output  label\n",
      "0  补贴主要针对司机端。\\n记者昨日从快的打车获悉，针对司机端的补贴将在今天正式实施：早上7点至...      1\n",
      "1  合作以及教育交流等方面有着广阔的合作空间，5月20日，省委统战部、省环保厅和省外事办在贵阳联...      1\n",
      "2  环绕希望对大家有用！\\n这9个基本功，99%的家长不会教孩子！\\n这9个基本功，99%的家长...      1\n"
     ]
    }
   ],
   "source": [
    "# 3. 加载千问数据\n",
    "def load_llm_data(file):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    outputs = [{\"output\": v, \"label\": 1} for k, v in data[\"output\"].items()]\n",
    "    return pd.DataFrame(outputs)\n",
    "\n",
    "llm_df = pd.concat([load_llm_data(f) for f in llm_files], ignore_index=True)\n",
    "print(\"🤖 千问生成数据样例：\")\n",
    "print(llm_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ea5a61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "# 1. 载入数据\n",
    "all_df = pd.concat([human_df, llm_df], ignore_index=True)\n",
    "dataset = Dataset.from_pandas(all_df.rename(columns={\"output\": \"text\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d381fc7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47a5c2493e8433eb44cd43bf1f4d09a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#model_name = \"bert-base-chinese\"\n",
    "model_name = \"hfl/chinese-bert-wwm-ext\"         # 比较大\n",
    "# 2. 数据预处理：分词与向量化\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# 使用map方法进行批处理分词\n",
    "dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a129cab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 3. 切分训练集和验证集\n",
    "train_dataset, eval_dataset = dataset.train_test_split(test_size=0.1).values()\n",
    "\n",
    "# 4. 设置模型和训练参数\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b35066f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86195\\AppData\\Local\\Temp\\ipykernel_26040\\1821734847.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# 5. 设置 Trainer 参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",  # 修改这里\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    report_to=\"none\",  # 关闭日志报告（如 TensorBoard）\n",
    "    dataloader_num_workers=4,  # 多线程加载数据，提高数据加载效率\n",
    "    fp16=False,  # CPU 禁用混合精度\n",
    "    save_total_limit=2,  # 最多保存 2 个 checkpoint\n",
    "    load_best_model_at_end=True,  # 加载最优模型\n",
    "    metric_for_best_model=\"eval_loss\",  # 根据评估 loss 判断最优模型\n",
    "    greater_is_better=False\n",
    ")\n",
    "# 6. Trainer 训练\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbe1c8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10125' max='10125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10125/10125 7:40:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.502700</td>\n",
       "      <td>0.544023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.405900</td>\n",
       "      <td>1.017501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.217200</td>\n",
       "      <td>1.438685</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\86195\\Anaconda\\envs\\Pytorch_py310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "d:\\86195\\Anaconda\\envs\\Pytorch_py310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10125, training_loss=0.37735411377895023, metrics={'train_runtime': 27680.4464, 'train_samples_per_second': 2.926, 'train_steps_per_second': 0.366, 'total_flos': 5334839758479360.0, 'train_loss': 0.37735411377895023, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练模型\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a429aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 模型已保存到 ./saved_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\86195\\Anaconda\\envs\\Pytorch_py310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评估结果：\n",
      "{'eval_loss': 0.5440232157707214, 'eval_runtime': 255.0984, 'eval_samples_per_second': 11.76, 'eval_steps_per_second': 1.47, 'epoch': 3.0}\n",
      "分类报告：\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.50      0.64      1534\n",
      "           1       0.64      0.93      0.76      1466\n",
      "\n",
      "    accuracy                           0.71      3000\n",
      "   macro avg       0.76      0.71      0.70      3000\n",
      "weighted avg       0.76      0.71      0.70      3000\n",
      "\n",
      "AUC 分数: 0.8447\n"
     ]
    }
   ],
   "source": [
    "# ✅ 保存最终模型和 tokenizer\n",
    "save_path = \"./saved_model\"\n",
    "trainer.save_model(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(f\"✅ 模型已保存到 {save_path}\")\n",
    "\n",
    "# 9. 评估模型\n",
    "results = trainer.evaluate()\n",
    "print(\"评估结果：\")\n",
    "print(results)\n",
    "\n",
    "# 10. 预测并输出分类报告\n",
    "predictions = trainer.predict(eval_dataset)\n",
    "pred_labels = predictions.predictions.argmax(axis=-1)\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "print(\"分类报告：\")\n",
    "print(classification_report(eval_dataset[\"label\"], pred_labels))\n",
    "\n",
    "# 计算 AUC\n",
    "roc_auc = roc_auc_score(eval_dataset[\"label\"], predictions.predictions[:, 1])\n",
    "print(f\"AUC 分数: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "478dd8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 human texts and 3000 GPT texts\n",
      "\n",
      "Sample human text:\n",
      " Environmental Factor\n",
      "Ensuring the safety of a newborn child becomes the primary and most important task of parents. Especially in the first year of life, the child is unable to protect himself from damaging environmental factors. Examples of such harmful effects may be air and water pollution, excessive sun exposure, or dust and chemicals. This work analyzes the harmful environmental factors for a child under one year old and offers health promotion to protect newborns from harm.\n",
      "Sources emphasi\n",
      "\n",
      "Sample GPT text:\n",
      " Introduction:\n",
      "Dyslexia is a learning disorder that affects numerous students, making it challenging for them to read, spell, and write. With its profound impact on academic performance, understanding the effects of dyslexia on children and exploring specialized interventions becomes crucial to ensure their success in school.\n",
      "Body Paragraphs:\n",
      "Effects of Dyslexia:\n",
      "Dyslexia can have various effects on children, both academically and emotionally. From an academic standpoint, dyslexic students often \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments\n",
    ")\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "def load_texts_from_folders(base_folder, subfolders):\n",
    "    texts = []\n",
    "    for subfolder in subfolders:\n",
    "        folder_path = os.path.join(base_folder, subfolder)\n",
    "        file_list = [f for f in os.listdir(folder_path) if f.endswith(\".txt\")]\n",
    "        file_list = sorted(file_list, key=lambda x: int(x.split(\".\")[0]))\n",
    "        for file_name in file_list:\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                texts.append(f.read())\n",
    "    return texts\n",
    "\n",
    "# 加载人类写的文本\n",
    "human_texts = load_texts_from_folders(\"ghostbuster-data/essay\", [\"human\"])\n",
    "\n",
    "# 加载所有GPT生成的文本（从三个不同的子目录）\n",
    "gpt_texts = load_texts_from_folders(\"ghostbuster-data/essay\", [\"gpt\", \"gpt_writing\", \"gpt_semantic\"])\n",
    "\n",
    "# 显示一些样本\n",
    "print(f\"Loaded {len(human_texts)} human texts and {len(gpt_texts)} GPT texts\")\n",
    "print(\"\\nSample human text:\\n\", human_texts[299][:500])\n",
    "print(\"\\nSample GPT text:\\n\", gpt_texts[2990][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "037eb839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 构建 DataFrame\n",
    "texts = human_texts + gpt_texts\n",
    "labels = [0] * len(human_texts) + [1] * len(gpt_texts)  # 0: Human, 1: GPT\n",
    "combined = list(zip(texts, labels))\n",
    "random.shuffle(combined)\n",
    "texts, labels = zip(*combined)\n",
    "df = pd.DataFrame({\"text\": texts, \"label\": labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f909eddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3600\n"
     ]
    }
   ],
   "source": [
    "# 3. 转换为 HuggingFace Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c9cf8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d1d6d6019e4bdfa9a0f5ccae842513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2838cec1a4e44f7b6cb4c13590301b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 4. 加载 tokenizer 和模型\n",
    "model_name = \"roberta-base\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=512)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "eval_dataset = eval_dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db1f4f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 5. 加载模型\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4425544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86195\\AppData\\Local\\Temp\\ipykernel_33268\\1934617829.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# 6. 设置 TrainingArguments（自动保存模型）\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert-eng-results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",  # 自动保存最佳模型\n",
    "    save_total_limit=2,     # 最多保留2个模型快照\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./bert-eng-logs\",\n",
    "    logging_steps=50,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    report_to=\"none\",\n",
    "    dataloader_num_workers=4,\n",
    "    fp16=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",  # 根据最小loss保存最佳模型\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "# 7. 定义 Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27572582",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\86195\\Anaconda\\envs\\Pytorch_py310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1350' max='1350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1350/1350 9:14:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>0.000336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.012816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.050201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\86195\\Anaconda\\envs\\Pytorch_py310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "d:\\86195\\Anaconda\\envs\\Pytorch_py310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "d:\\86195\\Anaconda\\envs\\Pytorch_py310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评估结果：\n",
      "{'eval_loss': 0.18355199587531388, 'eval_runtime': 161.9234, 'eval_samples_per_second': 2.47, 'eval_steps_per_second': 0.309, 'epoch': 3.0}\n",
      "分类报告：\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.83      0.79        96\n",
      "           1       0.94      0.92      0.93       304\n",
      "\n",
      "    accuracy                           0.89       400\n",
      "   macro avg       0.85      0.88      0.86       400\n",
      "weighted avg       0.90      0.89      0.89       400\n",
      "\n",
      "AUC 分数: 0.9320\n"
     ]
    }
   ],
   "source": [
    "# 8. 训练模型\n",
    "trainer.train()\n",
    "\n",
    "# 9. 评估模型\n",
    "results = trainer.evaluate()\n",
    "print(\"评估结果：\")\n",
    "print(results)\n",
    "\n",
    "# 10. 预测与分类报告\n",
    "predictions = trainer.predict(eval_dataset)\n",
    "pred_labels = predictions.predictions.argmax(axis=-1)\n",
    "print(\"分类报告：\")\n",
    "print(classification_report(eval_dataset[\"label\"], pred_labels))\n",
    "roc_auc = roc_auc_score(eval_dataset[\"label\"], predictions.predictions[:, 1])\n",
    "print(f\"AUC 分数: {roc_auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
