{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56450427",
   "metadata": {},
   "source": [
    "# ç®€å•äºŒåˆ†+tfidfï¼Œä¸­æ–‡æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8142d26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… äººå·¥æ–‡ä»¶: ['face2_zh_json/human/zh_unicode\\\\news-zh.json', 'face2_zh_json/human/zh_unicode\\\\webnovel.json', 'face2_zh_json/human/zh_unicode\\\\wiki-zh.json']\n",
      "âœ… åƒé—®æ–‡ä»¶: ['face2_zh_json/generated/zh_qwen2\\\\news-zh.qwen2-72b-base.json', 'face2_zh_json/generated/zh_qwen2\\\\webnovel.qwen2-72b-base.json', 'face2_zh_json/generated/zh_qwen2\\\\wiki-zh.qwen2-72b-base.json']\n"
     ]
    }
   ],
   "source": [
    "# 0. å¯¼å…¥åº“\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "# 1. è¯»å–æ•°æ®é›†è·¯å¾„\n",
    "human_path = \"face2_zh_json/human/zh_unicode\"\n",
    "llm_path = \"face2_zh_json/generated/zh_qwen2\"\n",
    "\n",
    "human_files = [os.path.join(human_path, f) for f in os.listdir(human_path)]\n",
    "llm_files = [os.path.join(llm_path, f) for f in os.listdir(llm_path)]\n",
    "\n",
    "print(\"âœ… äººå·¥æ–‡ä»¶:\", human_files[:3])\n",
    "print(\"âœ… åƒé—®æ–‡ä»¶:\", llm_files[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4436cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‘€ äººå·¥æ•°æ®æ ·ä¾‹ï¼š\n",
      "                                              output  label\n",
      "0  è¡¥è´´åï¼Œå˜æˆäº†ä»¥ç§¯åˆ†ã€æŠ½å¥–ç­‰å½¢å¼ä¸ºä¸»çš„â€œæš—è¡¥â€ã€‚ä¸€ç»„å…¬å¼€çš„æ•°æ®æ˜¾ç¤ºï¼Œåœè¡¥åçš„â€œæ»´æ»´æ‰“è½¦â€æ—¥å‡...      0\n",
      "1  åŸ¹è®­ã€æŠ•èèµ„ç­‰æ–¹é¢æœ‰ç€å·¨å¤§çš„åˆä½œç©ºé—´ï¼Œä¸ºç§¯ææ¨åŠ¨æˆ‘çœä¸ä¸¹éº¦çš„å‹å¥½äº¤æµï¼Œä¿ƒè¿›åŒè¾¹ç»è´¸æŠ•èµ„åˆä½œï¼Œ...      0\n",
      "2                               ç¯ç»“ä½ å–œæ¬¢å“ªç§å‘¢ï¼Ÿè§‰å¾—ä¸é”™ï¼Œè¯·ç‚¹èµâ†“â†“â†“      0\n"
     ]
    }
   ],
   "source": [
    "# 2. åŠ è½½äººå·¥æ•°æ®\n",
    "def load_human_data(file):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    df = pd.DataFrame(data)\n",
    "    df[\"label\"] = 0  # äººå·¥æ ‡è®°ä¸º0\n",
    "    return df[[\"output\", \"label\"]]\n",
    "\n",
    "human_df = pd.concat([load_human_data(f) for f in human_files], ignore_index=True)\n",
    "print(\"ğŸ‘€ äººå·¥æ•°æ®æ ·ä¾‹ï¼š\")\n",
    "print(human_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "468e948e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– åƒé—®ç”Ÿæˆæ•°æ®æ ·ä¾‹ï¼š\n",
      "                                              output  label\n",
      "0  è¡¥è´´ä¸»è¦é’ˆå¯¹å¸æœºç«¯ã€‚\\nè®°è€…æ˜¨æ—¥ä»å¿«çš„æ‰“è½¦è·æ‚‰ï¼Œé’ˆå¯¹å¸æœºç«¯çš„è¡¥è´´å°†åœ¨ä»Šå¤©æ­£å¼å®æ–½ï¼šæ—©ä¸Š7ç‚¹è‡³...      1\n",
      "1  åˆä½œä»¥åŠæ•™è‚²äº¤æµç­‰æ–¹é¢æœ‰ç€å¹¿é˜”çš„åˆä½œç©ºé—´ï¼Œ5æœˆ20æ—¥ï¼Œçœå§”ç»Ÿæˆ˜éƒ¨ã€çœç¯ä¿å…å’Œçœå¤–äº‹åŠåœ¨è´µé˜³è”...      1\n",
      "2  ç¯ç»•å¸Œæœ›å¯¹å¤§å®¶æœ‰ç”¨ï¼\\nè¿™9ä¸ªåŸºæœ¬åŠŸï¼Œ99%çš„å®¶é•¿ä¸ä¼šæ•™å­©å­ï¼\\nè¿™9ä¸ªåŸºæœ¬åŠŸï¼Œ99%çš„å®¶é•¿...      1\n"
     ]
    }
   ],
   "source": [
    "# 3. åŠ è½½åƒé—®æ•°æ®\n",
    "def load_llm_data(file):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    outputs = [{\"output\": v, \"label\": 1} for k, v in data[\"output\"].items()]\n",
    "    return pd.DataFrame(outputs)\n",
    "\n",
    "llm_df = pd.concat([load_llm_data(f) for f in llm_files], ignore_index=True)\n",
    "print(\"ğŸ¤– åƒé—®ç”Ÿæˆæ•°æ®æ ·ä¾‹ï¼š\")\n",
    "print(llm_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ea5a61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "# 1. è½½å…¥æ•°æ®\n",
    "all_df = pd.concat([human_df, llm_df], ignore_index=True)\n",
    "dataset = Dataset.from_pandas(all_df.rename(columns={\"output\": \"text\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d381fc7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47a5c2493e8433eb44cd43bf1f4d09a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#model_name = \"bert-base-chinese\"\n",
    "model_name = \"hfl/chinese-bert-wwm-ext\"         # æ¯”è¾ƒå¤§\n",
    "# 2. æ•°æ®é¢„å¤„ç†ï¼šåˆ†è¯ä¸å‘é‡åŒ–\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# ä½¿ç”¨mapæ–¹æ³•è¿›è¡Œæ‰¹å¤„ç†åˆ†è¯\n",
    "dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a129cab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 3. åˆ‡åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†\n",
    "train_dataset, eval_dataset = dataset.train_test_split(test_size=0.1).values()\n",
    "\n",
    "# 4. è®¾ç½®æ¨¡å‹å’Œè®­ç»ƒå‚æ•°\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b35066f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86195\\AppData\\Local\\Temp\\ipykernel_26040\\1821734847.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# 5. è®¾ç½® Trainer å‚æ•°\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",  # ä¿®æ”¹è¿™é‡Œ\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    report_to=\"none\",  # å…³é—­æ—¥å¿—æŠ¥å‘Šï¼ˆå¦‚ TensorBoardï¼‰\n",
    "    dataloader_num_workers=4,  # å¤šçº¿ç¨‹åŠ è½½æ•°æ®ï¼Œæé«˜æ•°æ®åŠ è½½æ•ˆç‡\n",
    "    fp16=False,  # CPU ç¦ç”¨æ··åˆç²¾åº¦\n",
    "    save_total_limit=2,  # æœ€å¤šä¿å­˜ 2 ä¸ª checkpoint\n",
    "    load_best_model_at_end=True,  # åŠ è½½æœ€ä¼˜æ¨¡å‹\n",
    "    metric_for_best_model=\"eval_loss\",  # æ ¹æ®è¯„ä¼° loss åˆ¤æ–­æœ€ä¼˜æ¨¡å‹\n",
    "    greater_is_better=False\n",
    ")\n",
    "# 6. Trainer è®­ç»ƒ\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbe1c8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10125' max='10125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10125/10125 7:40:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.502700</td>\n",
       "      <td>0.544023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.405900</td>\n",
       "      <td>1.017501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.217200</td>\n",
       "      <td>1.438685</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\86195\\Anaconda\\envs\\Pytorch_py310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "d:\\86195\\Anaconda\\envs\\Pytorch_py310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10125, training_loss=0.37735411377895023, metrics={'train_runtime': 27680.4464, 'train_samples_per_second': 2.926, 'train_steps_per_second': 0.366, 'total_flos': 5334839758479360.0, 'train_loss': 0.37735411377895023, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# è®­ç»ƒæ¨¡å‹\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a429aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ¨¡å‹å·²ä¿å­˜åˆ° ./saved_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\86195\\Anaconda\\envs\\Pytorch_py310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¯„ä¼°ç»“æœï¼š\n",
      "{'eval_loss': 0.5440232157707214, 'eval_runtime': 255.0984, 'eval_samples_per_second': 11.76, 'eval_steps_per_second': 1.47, 'epoch': 3.0}\n",
      "åˆ†ç±»æŠ¥å‘Šï¼š\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.50      0.64      1534\n",
      "           1       0.64      0.93      0.76      1466\n",
      "\n",
      "    accuracy                           0.71      3000\n",
      "   macro avg       0.76      0.71      0.70      3000\n",
      "weighted avg       0.76      0.71      0.70      3000\n",
      "\n",
      "AUC åˆ†æ•°: 0.8447\n"
     ]
    }
   ],
   "source": [
    "# âœ… ä¿å­˜æœ€ç»ˆæ¨¡å‹å’Œ tokenizer\n",
    "save_path = \"./saved_model\"\n",
    "trainer.save_model(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(f\"âœ… æ¨¡å‹å·²ä¿å­˜åˆ° {save_path}\")\n",
    "\n",
    "# 9. è¯„ä¼°æ¨¡å‹\n",
    "results = trainer.evaluate()\n",
    "print(\"è¯„ä¼°ç»“æœï¼š\")\n",
    "print(results)\n",
    "\n",
    "# 10. é¢„æµ‹å¹¶è¾“å‡ºåˆ†ç±»æŠ¥å‘Š\n",
    "predictions = trainer.predict(eval_dataset)\n",
    "pred_labels = predictions.predictions.argmax(axis=-1)\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "print(\"åˆ†ç±»æŠ¥å‘Šï¼š\")\n",
    "print(classification_report(eval_dataset[\"label\"], pred_labels))\n",
    "\n",
    "# è®¡ç®— AUC\n",
    "roc_auc = roc_auc_score(eval_dataset[\"label\"], predictions.predictions[:, 1])\n",
    "print(f\"AUC åˆ†æ•°: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "478dd8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 human texts and 3000 GPT texts\n",
      "\n",
      "Sample human text:\n",
      " Environmental Factor\n",
      "Ensuring the safety of a newborn child becomes the primary and most important task of parents. Especially in the first year of life, the child is unable to protect himself from damaging environmental factors. Examples of such harmful effects may be air and water pollution, excessive sun exposure, or dust and chemicals. This work analyzes the harmful environmental factors for a child under one year old and offers health promotion to protect newborns from harm.\n",
      "Sources emphasi\n",
      "\n",
      "Sample GPT text:\n",
      " Introduction:\n",
      "Dyslexia is a learning disorder that affects numerous students, making it challenging for them to read, spell, and write. With its profound impact on academic performance, understanding the effects of dyslexia on children and exploring specialized interventions becomes crucial to ensure their success in school.\n",
      "Body Paragraphs:\n",
      "Effects of Dyslexia:\n",
      "Dyslexia can have various effects on children, both academically and emotionally. From an academic standpoint, dyslexic students often \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments\n",
    ")\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "def load_texts_from_folders(base_folder, subfolders):\n",
    "    texts = []\n",
    "    for subfolder in subfolders:\n",
    "        folder_path = os.path.join(base_folder, subfolder)\n",
    "        file_list = [f for f in os.listdir(folder_path) if f.endswith(\".txt\")]\n",
    "        file_list = sorted(file_list, key=lambda x: int(x.split(\".\")[0]))\n",
    "        for file_name in file_list:\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                texts.append(f.read())\n",
    "    return texts\n",
    "\n",
    "# åŠ è½½äººç±»å†™çš„æ–‡æœ¬\n",
    "human_texts = load_texts_from_folders(\"ghostbuster-data/essay\", [\"human\"])\n",
    "\n",
    "# åŠ è½½æ‰€æœ‰GPTç”Ÿæˆçš„æ–‡æœ¬ï¼ˆä»ä¸‰ä¸ªä¸åŒçš„å­ç›®å½•ï¼‰\n",
    "gpt_texts = load_texts_from_folders(\"ghostbuster-data/essay\", [\"gpt\", \"gpt_writing\", \"gpt_semantic\"])\n",
    "\n",
    "# æ˜¾ç¤ºä¸€äº›æ ·æœ¬\n",
    "print(f\"Loaded {len(human_texts)} human texts and {len(gpt_texts)} GPT texts\")\n",
    "print(\"\\nSample human text:\\n\", human_texts[299][:500])\n",
    "print(\"\\nSample GPT text:\\n\", gpt_texts[2990][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "037eb839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. æ„å»º DataFrame\n",
    "texts = human_texts + gpt_texts\n",
    "labels = [0] * len(human_texts) + [1] * len(gpt_texts)  # 0: Human, 1: GPT\n",
    "combined = list(zip(texts, labels))\n",
    "random.shuffle(combined)\n",
    "texts, labels = zip(*combined)\n",
    "df = pd.DataFrame({\"text\": texts, \"label\": labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f909eddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3600\n"
     ]
    }
   ],
   "source": [
    "# 3. è½¬æ¢ä¸º HuggingFace Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c9cf8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d1d6d6019e4bdfa9a0f5ccae842513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2838cec1a4e44f7b6cb4c13590301b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 4. åŠ è½½ tokenizer å’Œæ¨¡å‹\n",
    "model_name = \"roberta-base\"  \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=512)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "eval_dataset = eval_dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db1f4f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 5. åŠ è½½æ¨¡å‹\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4425544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86195\\AppData\\Local\\Temp\\ipykernel_33268\\1934617829.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# 6. è®¾ç½® TrainingArgumentsï¼ˆè‡ªåŠ¨ä¿å­˜æ¨¡å‹ï¼‰\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert-eng-results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",  # è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "    save_total_limit=2,     # æœ€å¤šä¿ç•™2ä¸ªæ¨¡å‹å¿«ç…§\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./bert-eng-logs\",\n",
    "    logging_steps=50,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    report_to=\"none\",\n",
    "    dataloader_num_workers=4,\n",
    "    fp16=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",  # æ ¹æ®æœ€å°lossä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "# 7. å®šä¹‰ Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27572582",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\86195\\Anaconda\\envs\\Pytorch_py310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1350' max='1350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1350/1350 9:14:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.048200</td>\n",
       "      <td>0.000336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.012816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.050201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\86195\\Anaconda\\envs\\Pytorch_py310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "d:\\86195\\Anaconda\\envs\\Pytorch_py310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "d:\\86195\\Anaconda\\envs\\Pytorch_py310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¯„ä¼°ç»“æœï¼š\n",
      "{'eval_loss': 0.18355199587531388, 'eval_runtime': 161.9234, 'eval_samples_per_second': 2.47, 'eval_steps_per_second': 0.309, 'epoch': 3.0}\n",
      "åˆ†ç±»æŠ¥å‘Šï¼š\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.83      0.79        96\n",
      "           1       0.94      0.92      0.93       304\n",
      "\n",
      "    accuracy                           0.89       400\n",
      "   macro avg       0.85      0.88      0.86       400\n",
      "weighted avg       0.90      0.89      0.89       400\n",
      "\n",
      "AUC åˆ†æ•°: 0.9320\n"
     ]
    }
   ],
   "source": [
    "# 8. è®­ç»ƒæ¨¡å‹\n",
    "trainer.train()\n",
    "\n",
    "# 9. è¯„ä¼°æ¨¡å‹\n",
    "results = trainer.evaluate()\n",
    "print(\"è¯„ä¼°ç»“æœï¼š\")\n",
    "print(results)\n",
    "\n",
    "# 10. é¢„æµ‹ä¸åˆ†ç±»æŠ¥å‘Š\n",
    "predictions = trainer.predict(eval_dataset)\n",
    "pred_labels = predictions.predictions.argmax(axis=-1)\n",
    "print(\"åˆ†ç±»æŠ¥å‘Šï¼š\")\n",
    "print(classification_report(eval_dataset[\"label\"], pred_labels))\n",
    "roc_auc = roc_auc_score(eval_dataset[\"label\"], predictions.predictions[:, 1])\n",
    "print(f\"AUC åˆ†æ•°: {roc_auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
