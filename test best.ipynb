{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56450427",
   "metadata": {},
   "source": [
    "# 简单二分+tfidf，中文模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8142d26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 人工文件: ['face2_zh_json/human/zh_unicode\\\\news-zh.json', 'face2_zh_json/human/zh_unicode\\\\webnovel.json', 'face2_zh_json/human/zh_unicode\\\\wiki-zh.json']\n",
      "✅ 千问文件: ['face2_zh_json/generated/zh_qwen2\\\\news-zh.qwen2-72b-base.json', 'face2_zh_json/generated/zh_qwen2\\\\webnovel.qwen2-72b-base.json', 'face2_zh_json/generated/zh_qwen2\\\\wiki-zh.qwen2-72b-base.json']\n"
     ]
    }
   ],
   "source": [
    "# 0. 导入库\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "# 1. 读取数据集路径\n",
    "human_path = \"face2_zh_json/human/zh_unicode\"\n",
    "llm_path = \"face2_zh_json/generated/zh_qwen2\"\n",
    "\n",
    "human_files = [os.path.join(human_path, f) for f in os.listdir(human_path)]\n",
    "llm_files = [os.path.join(llm_path, f) for f in os.listdir(llm_path)]\n",
    "\n",
    "print(\"✅ 人工文件:\", human_files[:3])\n",
    "print(\"✅ 千问文件:\", llm_files[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4436cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👀 人工数据样例：\n",
      "                                              output  label\n",
      "0  补贴后，变成了以积分、抽奖等形式为主的“暗补”。一组公开的数据显示，停补后的“滴滴打车”日均...      0\n",
      "1  培训、投融资等方面有着巨大的合作空间，为积极推动我省与丹麦的友好交流，促进双边经贸投资合作，...      0\n",
      "2                               环结你喜欢哪种呢？觉得不错，请点赞↓↓↓      0\n"
     ]
    }
   ],
   "source": [
    "# 2. 加载人工数据\n",
    "def load_human_data(file):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    df = pd.DataFrame(data)\n",
    "    df[\"label\"] = 0  # 人工标记为0\n",
    "    return df[[\"output\", \"label\"]]\n",
    "\n",
    "human_df = pd.concat([load_human_data(f) for f in human_files], ignore_index=True)\n",
    "print(\"👀 人工数据样例：\")\n",
    "print(human_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "468e948e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 千问生成数据样例：\n",
      "                                              output  label\n",
      "0  补贴主要针对司机端。\\n记者昨日从快的打车获悉，针对司机端的补贴将在今天正式实施：早上7点至...      1\n",
      "1  合作以及教育交流等方面有着广阔的合作空间，5月20日，省委统战部、省环保厅和省外事办在贵阳联...      1\n",
      "2  环绕希望对大家有用！\\n这9个基本功，99%的家长不会教孩子！\\n这9个基本功，99%的家长...      1\n"
     ]
    }
   ],
   "source": [
    "# 3. 加载千问数据\n",
    "def load_llm_data(file):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    outputs = [{\"output\": v, \"label\": 1} for k, v in data[\"output\"].items()]\n",
    "    return pd.DataFrame(outputs)\n",
    "\n",
    "llm_df = pd.concat([load_llm_data(f) for f in llm_files], ignore_index=True)\n",
    "print(\"🤖 千问生成数据样例：\")\n",
    "print(llm_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ea5a61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "# 1. 载入数据\n",
    "all_df = pd.concat([human_df, llm_df], ignore_index=True)\n",
    "dataset = Dataset.from_pandas(all_df.rename(columns={\"output\": \"text\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d381fc7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47a5c2493e8433eb44cd43bf1f4d09a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"bert-base-chinese\"\n",
    "#model_name = \"hfl/chinese-bert-wwm-ext\"         # 比较大\n",
    "# 2. 数据预处理：分词与向量化\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# 使用map方法进行批处理分词\n",
    "dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a129cab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 3. 切分训练集和验证集\n",
    "train_dataset, eval_dataset = dataset.train_test_split(test_size=0.1).values()\n",
    "\n",
    "# 4. 设置模型和训练参数\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b35066f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86195\\AppData\\Local\\Temp\\ipykernel_26040\\1821734847.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# 5. 设置 Trainer 参数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",  # 修改这里\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    report_to=\"none\",  # 关闭日志报告（如 TensorBoard）\n",
    "    dataloader_num_workers=4,  # 多线程加载数据，提高数据加载效率\n",
    "    fp16=False,  # CPU 禁用混合精度\n",
    "    save_total_limit=2,  # 最多保存 2 个 checkpoint\n",
    "    load_best_model_at_end=True,  # 加载最优模型\n",
    "    metric_for_best_model=\"eval_loss\",  # 根据评估 loss 判断最优模型\n",
    "    greater_is_better=False\n",
    ")\n",
    "# 6. Trainer 训练\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbe1c8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10125' max='10125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10125/10125 7:40:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.502700</td>\n",
       "      <td>0.544023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.405900</td>\n",
       "      <td>1.017501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.217200</td>\n",
       "      <td>1.438685</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\86195\\Anaconda\\envs\\Pytorch_py310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "d:\\86195\\Anaconda\\envs\\Pytorch_py310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10125, training_loss=0.37735411377895023, metrics={'train_runtime': 27680.4464, 'train_samples_per_second': 2.926, 'train_steps_per_second': 0.366, 'total_flos': 5334839758479360.0, 'train_loss': 0.37735411377895023, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练模型\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a429aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 模型已保存到 ./saved_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\86195\\Anaconda\\envs\\Pytorch_py310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评估结果：\n",
      "{'eval_loss': 0.5440232157707214, 'eval_runtime': 255.0984, 'eval_samples_per_second': 11.76, 'eval_steps_per_second': 1.47, 'epoch': 3.0}\n",
      "分类报告：\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.50      0.64      1534\n",
      "           1       0.64      0.93      0.76      1466\n",
      "\n",
      "    accuracy                           0.71      3000\n",
      "   macro avg       0.76      0.71      0.70      3000\n",
      "weighted avg       0.76      0.71      0.70      3000\n",
      "\n",
      "AUC 分数: 0.8447\n"
     ]
    }
   ],
   "source": [
    "# ✅ 保存最终模型和 tokenizer\n",
    "save_path = \"./saved_model\"\n",
    "trainer.save_model(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(f\"✅ 模型已保存到 {save_path}\")\n",
    "\n",
    "# 9. 评估模型\n",
    "results = trainer.evaluate()\n",
    "print(\"评估结果：\")\n",
    "print(results)\n",
    "\n",
    "# 10. 预测并输出分类报告\n",
    "predictions = trainer.predict(eval_dataset)\n",
    "pred_labels = predictions.predictions.argmax(axis=-1)\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "print(\"分类报告：\")\n",
    "print(classification_report(eval_dataset[\"label\"], pred_labels))\n",
    "\n",
    "# 计算 AUC\n",
    "roc_auc = roc_auc_score(eval_dataset[\"label\"], predictions.predictions[:, 1])\n",
    "print(f\"AUC 分数: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "478dd8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample human text:\n",
      " Supervision is a process of knowledge exchange, social experience, and psychological support received by trainees in work, career, and professional development. It includes informal communication, usually between two people, over a long period, between an employee who has a large amount of relevant knowledge, wisdom, or experience, and an employee or student who has these qualities to a lesser extent. In this regard, the supervisors must have particular traits and specific demeanor to succeed in\n",
      "\n",
      "Sample GPT text:\n",
      " Introduction:\n",
      "The film \"12 Years a Slave\" serves as a poignant depiction of the harrowing realities of slavery, shedding light on themes of collectivism and individualism. Through its exploration of these themes, the movie effectively portrays slavery as a widespread issue with far-reaching consequences. Moreover, it vividly portrays instances of prejudice, generalizations, stereotyping, and discrimination against black people, showcasing their profound impact on the experiences of the character\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments\n",
    ")\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "# 2. Load all .txt files\n",
    "def load_texts_from_folder(folder_path):\n",
    "    texts = []\n",
    "    file_list = [f for f in os.listdir(folder_path) if f.endswith(\".txt\")]  # Only keep .txt files\n",
    "    file_list = sorted(file_list, key=lambda x: int(x.split(\".\")[0]))  # Sort by the numeric part of filename\n",
    "    for file_name in file_list:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            texts.append(f.read())\n",
    "    return texts\n",
    "\n",
    "human_texts = load_texts_from_folder(\"ghostbuster-data/essay/human\")\n",
    "gpt_texts = load_texts_from_folder(\"ghostbuster-data/essay/gpt\")\n",
    "\n",
    "# Show a few samples\n",
    "print(\"Sample human text:\\n\", human_texts[999][:500])\n",
    "print(\"\\nSample GPT text:\\n\", gpt_texts[0][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "037eb839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 构建 DataFrame\n",
    "texts = human_texts + gpt_texts\n",
    "labels = [0] * len(human_texts) + [1] * len(gpt_texts)  # 0: Human, 1: GPT\n",
    "combined = list(zip(texts, labels))\n",
    "random.shuffle(combined)\n",
    "texts, labels = zip(*combined)\n",
    "df = pd.DataFrame({\"text\": texts, \"label\": labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f909eddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 转换为 HuggingFace Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c9cf8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa0a79ceded7459d8c5b3dbeb88d5a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "546dafb8a9b04fa2b0a694d33dc3d585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 4. 加载 tokenizer 和模型\n",
    "model_name = \"bert-base-uncased\"  # 可替换为更强的如 'roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=512)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "eval_dataset = eval_dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db1f4f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "260273b2262b4064b742453118c75cd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 5. 加载模型\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4425544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86195\\AppData\\Local\\Temp\\ipykernel_18760\\1934617829.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# 6. 设置 TrainingArguments（自动保存模型）\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert-eng-results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",  # 自动保存最佳模型\n",
    "    save_total_limit=2,     # 最多保留2个模型快照\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./bert-eng-logs\",\n",
    "    logging_steps=50,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    report_to=\"none\",\n",
    "    dataloader_num_workers=4,\n",
    "    fp16=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",  # 根据最小loss保存最佳模型\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "# 7. 定义 Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27572582",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\86195\\Anaconda\\envs\\Pytorch_py310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 2:21:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.091100</td>\n",
       "      <td>0.013253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.054682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.037735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\86195\\Anaconda\\envs\\Pytorch_py310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "d:\\86195\\Anaconda\\envs\\Pytorch_py310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "d:\\86195\\Anaconda\\envs\\Pytorch_py310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "评估结果：\n",
      "{'eval_loss': 0.013252943754196167, 'eval_runtime': 162.4564, 'eval_samples_per_second': 2.462, 'eval_steps_per_second': 0.308, 'epoch': 3.0}\n",
      "分类报告：\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       200\n",
      "           1       1.00      0.99      1.00       200\n",
      "\n",
      "    accuracy                           1.00       400\n",
      "   macro avg       1.00      1.00      1.00       400\n",
      "weighted avg       1.00      1.00      1.00       400\n",
      "\n",
      "AUC 分数: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# 8. 训练模型\n",
    "trainer.train()\n",
    "\n",
    "# 9. 评估模型\n",
    "results = trainer.evaluate()\n",
    "print(\"评估结果：\")\n",
    "print(results)\n",
    "\n",
    "# 10. 预测与分类报告\n",
    "predictions = trainer.predict(eval_dataset)\n",
    "pred_labels = predictions.predictions.argmax(axis=-1)\n",
    "print(\"分类报告：\")\n",
    "print(classification_report(eval_dataset[\"label\"], pred_labels))\n",
    "roc_auc = roc_auc_score(eval_dataset[\"label\"], predictions.predictions[:, 1])\n",
    "print(f\"AUC 分数: {roc_auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
