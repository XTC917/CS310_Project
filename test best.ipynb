{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56450427",
   "metadata": {},
   "source": [
    "# ç®€å•äºŒåˆ†+tfidfï¼Œä¸­æ–‡æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8142d26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… äººå·¥æ–‡ä»¶: ['face2_zh_json/human/zh_unicode\\\\news-zh.json', 'face2_zh_json/human/zh_unicode\\\\webnovel.json', 'face2_zh_json/human/zh_unicode\\\\wiki-zh.json']\n",
      "âœ… åƒé—®æ–‡ä»¶: ['face2_zh_json/generated/zh_qwen2\\\\news-zh.qwen2-72b-base.json', 'face2_zh_json/generated/zh_qwen2\\\\webnovel.qwen2-72b-base.json', 'face2_zh_json/generated/zh_qwen2\\\\wiki-zh.qwen2-72b-base.json']\n"
     ]
    }
   ],
   "source": [
    "# 0. å¯¼å…¥åº“\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import jieba\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "# 1. è¯»å–æ•°æ®é›†è·¯å¾„\n",
    "human_path = \"face2_zh_json/human/zh_unicode\"\n",
    "llm_path = \"face2_zh_json/generated/zh_qwen2\"\n",
    "\n",
    "human_files = [os.path.join(human_path, f) for f in os.listdir(human_path)]\n",
    "llm_files = [os.path.join(llm_path, f) for f in os.listdir(llm_path)]\n",
    "\n",
    "print(\"âœ… äººå·¥æ–‡ä»¶:\", human_files[:3])\n",
    "print(\"âœ… åƒé—®æ–‡ä»¶:\", llm_files[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4436cec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‘€ äººå·¥æ•°æ®æ ·ä¾‹ï¼š\n",
      "                                              output  label\n",
      "0  è¡¥è´´åï¼Œå˜æˆäº†ä»¥ç§¯åˆ†ã€æŠ½å¥–ç­‰å½¢å¼ä¸ºä¸»çš„â€œæš—è¡¥â€ã€‚ä¸€ç»„å…¬å¼€çš„æ•°æ®æ˜¾ç¤ºï¼Œåœè¡¥åçš„â€œæ»´æ»´æ‰“è½¦â€æ—¥å‡...      0\n",
      "1  åŸ¹è®­ã€æŠ•èèµ„ç­‰æ–¹é¢æœ‰ç€å·¨å¤§çš„åˆä½œç©ºé—´ï¼Œä¸ºç§¯ææ¨åŠ¨æˆ‘çœä¸ä¸¹éº¦çš„å‹å¥½äº¤æµï¼Œä¿ƒè¿›åŒè¾¹ç»è´¸æŠ•èµ„åˆä½œï¼Œ...      0\n",
      "2                               ç¯ç»“ä½ å–œæ¬¢å“ªç§å‘¢ï¼Ÿè§‰å¾—ä¸é”™ï¼Œè¯·ç‚¹èµâ†“â†“â†“      0\n"
     ]
    }
   ],
   "source": [
    "# 2. åŠ è½½äººå·¥æ•°æ®\n",
    "def load_human_data(file):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    df = pd.DataFrame(data)\n",
    "    df[\"label\"] = 0  # äººå·¥æ ‡è®°ä¸º0\n",
    "    return df[[\"output\", \"label\"]]\n",
    "\n",
    "human_df = pd.concat([load_human_data(f) for f in human_files], ignore_index=True)\n",
    "print(\"ğŸ‘€ äººå·¥æ•°æ®æ ·ä¾‹ï¼š\")\n",
    "print(human_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "468e948e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– åƒé—®ç”Ÿæˆæ•°æ®æ ·ä¾‹ï¼š\n",
      "                                              output  label\n",
      "0  è¡¥è´´ä¸»è¦é’ˆå¯¹å¸æœºç«¯ã€‚\\nè®°è€…æ˜¨æ—¥ä»å¿«çš„æ‰“è½¦è·æ‚‰ï¼Œé’ˆå¯¹å¸æœºç«¯çš„è¡¥è´´å°†åœ¨ä»Šå¤©æ­£å¼å®æ–½ï¼šæ—©ä¸Š7ç‚¹è‡³...      1\n",
      "1  åˆä½œä»¥åŠæ•™è‚²äº¤æµç­‰æ–¹é¢æœ‰ç€å¹¿é˜”çš„åˆä½œç©ºé—´ï¼Œ5æœˆ20æ—¥ï¼Œçœå§”ç»Ÿæˆ˜éƒ¨ã€çœç¯ä¿å…å’Œçœå¤–äº‹åŠåœ¨è´µé˜³è”...      1\n",
      "2  ç¯ç»•å¸Œæœ›å¯¹å¤§å®¶æœ‰ç”¨ï¼\\nè¿™9ä¸ªåŸºæœ¬åŠŸï¼Œ99%çš„å®¶é•¿ä¸ä¼šæ•™å­©å­ï¼\\nè¿™9ä¸ªåŸºæœ¬åŠŸï¼Œ99%çš„å®¶é•¿...      1\n"
     ]
    }
   ],
   "source": [
    "# 3. åŠ è½½åƒé—®æ•°æ®\n",
    "def load_llm_data(file):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    outputs = [{\"output\": v, \"label\": 1} for k, v in data[\"output\"].items()]\n",
    "    return pd.DataFrame(outputs)\n",
    "\n",
    "llm_df = pd.concat([load_llm_data(f) for f in llm_files], ignore_index=True)\n",
    "print(\"ğŸ¤– åƒé—®ç”Ÿæˆæ•°æ®æ ·ä¾‹ï¼š\")\n",
    "print(llm_df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ea5a61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "# 1. è½½å…¥æ•°æ®\n",
    "all_df = pd.concat([human_df, llm_df], ignore_index=True)\n",
    "dataset = Dataset.from_pandas(all_df.rename(columns={\"output\": \"text\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d381fc7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c47a5c2493e8433eb44cd43bf1f4d09a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"bert-base-chinese\"\n",
    "#model_name = \"hfl/chinese-bert-wwm-ext\"         # æ¯”è¾ƒå¤§\n",
    "# 2. æ•°æ®é¢„å¤„ç†ï¼šåˆ†è¯ä¸å‘é‡åŒ–\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# ä½¿ç”¨mapæ–¹æ³•è¿›è¡Œæ‰¹å¤„ç†åˆ†è¯\n",
    "dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a129cab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 3. åˆ‡åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†\n",
    "train_dataset, eval_dataset = dataset.train_test_split(test_size=0.1).values()\n",
    "\n",
    "# 4. è®¾ç½®æ¨¡å‹å’Œè®­ç»ƒå‚æ•°\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b35066f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86195\\AppData\\Local\\Temp\\ipykernel_26040\\1821734847.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# 5. è®¾ç½® Trainer å‚æ•°\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",  # ä¿®æ”¹è¿™é‡Œ\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    report_to=\"none\",  # å…³é—­æ—¥å¿—æŠ¥å‘Šï¼ˆå¦‚ TensorBoardï¼‰\n",
    "    dataloader_num_workers=4,  # å¤šçº¿ç¨‹åŠ è½½æ•°æ®ï¼Œæé«˜æ•°æ®åŠ è½½æ•ˆç‡\n",
    "    fp16=False,  # CPU ç¦ç”¨æ··åˆç²¾åº¦\n",
    "    save_total_limit=2,  # æœ€å¤šä¿å­˜ 2 ä¸ª checkpoint\n",
    "    load_best_model_at_end=True,  # åŠ è½½æœ€ä¼˜æ¨¡å‹\n",
    "    metric_for_best_model=\"eval_loss\",  # æ ¹æ®è¯„ä¼° loss åˆ¤æ–­æœ€ä¼˜æ¨¡å‹\n",
    "    greater_is_better=False\n",
    ")\n",
    "# 6. Trainer è®­ç»ƒ\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbe1c8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10125' max='10125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10125/10125 7:40:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.502700</td>\n",
       "      <td>0.544023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.405900</td>\n",
       "      <td>1.017501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.217200</td>\n",
       "      <td>1.438685</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\86195\\Anaconda\\envs\\Pytorch_py310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "d:\\86195\\Anaconda\\envs\\Pytorch_py310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10125, training_loss=0.37735411377895023, metrics={'train_runtime': 27680.4464, 'train_samples_per_second': 2.926, 'train_steps_per_second': 0.366, 'total_flos': 5334839758479360.0, 'train_loss': 0.37735411377895023, 'epoch': 3.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# è®­ç»ƒæ¨¡å‹\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a429aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ¨¡å‹å·²ä¿å­˜åˆ° ./saved_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\86195\\Anaconda\\envs\\Pytorch_py310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¯„ä¼°ç»“æœï¼š\n",
      "{'eval_loss': 0.5440232157707214, 'eval_runtime': 255.0984, 'eval_samples_per_second': 11.76, 'eval_steps_per_second': 1.47, 'epoch': 3.0}\n",
      "åˆ†ç±»æŠ¥å‘Šï¼š\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.50      0.64      1534\n",
      "           1       0.64      0.93      0.76      1466\n",
      "\n",
      "    accuracy                           0.71      3000\n",
      "   macro avg       0.76      0.71      0.70      3000\n",
      "weighted avg       0.76      0.71      0.70      3000\n",
      "\n",
      "AUC åˆ†æ•°: 0.8447\n"
     ]
    }
   ],
   "source": [
    "# âœ… ä¿å­˜æœ€ç»ˆæ¨¡å‹å’Œ tokenizer\n",
    "save_path = \"./saved_model\"\n",
    "trainer.save_model(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "print(f\"âœ… æ¨¡å‹å·²ä¿å­˜åˆ° {save_path}\")\n",
    "\n",
    "# 9. è¯„ä¼°æ¨¡å‹\n",
    "results = trainer.evaluate()\n",
    "print(\"è¯„ä¼°ç»“æœï¼š\")\n",
    "print(results)\n",
    "\n",
    "# 10. é¢„æµ‹å¹¶è¾“å‡ºåˆ†ç±»æŠ¥å‘Š\n",
    "predictions = trainer.predict(eval_dataset)\n",
    "pred_labels = predictions.predictions.argmax(axis=-1)\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "print(\"åˆ†ç±»æŠ¥å‘Šï¼š\")\n",
    "print(classification_report(eval_dataset[\"label\"], pred_labels))\n",
    "\n",
    "# è®¡ç®— AUC\n",
    "roc_auc = roc_auc_score(eval_dataset[\"label\"], predictions.predictions[:, 1])\n",
    "print(f\"AUC åˆ†æ•°: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "478dd8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample human text:\n",
      " Supervision is a process of knowledge exchange, social experience, and psychological support received by trainees in work, career, and professional development. It includes informal communication, usually between two people, over a long period, between an employee who has a large amount of relevant knowledge, wisdom, or experience, and an employee or student who has these qualities to a lesser extent. In this regard, the supervisors must have particular traits and specific demeanor to succeed in\n",
      "\n",
      "Sample GPT text:\n",
      " Introduction:\n",
      "The film \"12 Years a Slave\" serves as a poignant depiction of the harrowing realities of slavery, shedding light on themes of collectivism and individualism. Through its exploration of these themes, the movie effectively portrays slavery as a widespread issue with far-reaching consequences. Moreover, it vividly portrays instances of prejudice, generalizations, stereotyping, and discrimination against black people, showcasing their profound impact on the experiences of the character\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    Trainer, TrainingArguments\n",
    ")\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "# 2. Load all .txt files\n",
    "def load_texts_from_folder(folder_path):\n",
    "    texts = []\n",
    "    file_list = [f for f in os.listdir(folder_path) if f.endswith(\".txt\")]  # Only keep .txt files\n",
    "    file_list = sorted(file_list, key=lambda x: int(x.split(\".\")[0]))  # Sort by the numeric part of filename\n",
    "    for file_name in file_list:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            texts.append(f.read())\n",
    "    return texts\n",
    "\n",
    "human_texts = load_texts_from_folder(\"ghostbuster-data/essay/human\")\n",
    "gpt_texts = load_texts_from_folder(\"ghostbuster-data/essay/gpt\")\n",
    "\n",
    "# Show a few samples\n",
    "print(\"Sample human text:\\n\", human_texts[999][:500])\n",
    "print(\"\\nSample GPT text:\\n\", gpt_texts[0][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "037eb839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. æ„å»º DataFrame\n",
    "texts = human_texts + gpt_texts\n",
    "labels = [0] * len(human_texts) + [1] * len(gpt_texts)  # 0: Human, 1: GPT\n",
    "combined = list(zip(texts, labels))\n",
    "random.shuffle(combined)\n",
    "texts, labels = zip(*combined)\n",
    "df = pd.DataFrame({\"text\": texts, \"label\": labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f909eddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. è½¬æ¢ä¸º HuggingFace Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c9cf8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa0a79ceded7459d8c5b3dbeb88d5a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "546dafb8a9b04fa2b0a694d33dc3d585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 4. åŠ è½½ tokenizer å’Œæ¨¡å‹\n",
    "model_name = \"bert-base-uncased\"  # å¯æ›¿æ¢ä¸ºæ›´å¼ºçš„å¦‚ 'roberta-base'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=True, truncation=True, max_length=512)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "eval_dataset = eval_dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db1f4f37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "260273b2262b4064b742453118c75cd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 5. åŠ è½½æ¨¡å‹\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4425544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\86195\\AppData\\Local\\Temp\\ipykernel_18760\\1934617829.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# 6. è®¾ç½® TrainingArgumentsï¼ˆè‡ªåŠ¨ä¿å­˜æ¨¡å‹ï¼‰\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert-eng-results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",  # è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "    save_total_limit=2,     # æœ€å¤šä¿ç•™2ä¸ªæ¨¡å‹å¿«ç…§\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./bert-eng-logs\",\n",
    "    logging_steps=50,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    report_to=\"none\",\n",
    "    dataloader_num_workers=4,\n",
    "    fp16=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",  # æ ¹æ®æœ€å°lossä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "# 7. å®šä¹‰ Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27572582",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\86195\\Anaconda\\envs\\Pytorch_py310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 2:21:33, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.091100</td>\n",
       "      <td>0.013253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.054682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.037735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\86195\\Anaconda\\envs\\Pytorch_py310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "d:\\86195\\Anaconda\\envs\\Pytorch_py310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "d:\\86195\\Anaconda\\envs\\Pytorch_py310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "è¯„ä¼°ç»“æœï¼š\n",
      "{'eval_loss': 0.013252943754196167, 'eval_runtime': 162.4564, 'eval_samples_per_second': 2.462, 'eval_steps_per_second': 0.308, 'epoch': 3.0}\n",
      "åˆ†ç±»æŠ¥å‘Šï¼š\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       200\n",
      "           1       1.00      0.99      1.00       200\n",
      "\n",
      "    accuracy                           1.00       400\n",
      "   macro avg       1.00      1.00      1.00       400\n",
      "weighted avg       1.00      1.00      1.00       400\n",
      "\n",
      "AUC åˆ†æ•°: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# 8. è®­ç»ƒæ¨¡å‹\n",
    "trainer.train()\n",
    "\n",
    "# 9. è¯„ä¼°æ¨¡å‹\n",
    "results = trainer.evaluate()\n",
    "print(\"è¯„ä¼°ç»“æœï¼š\")\n",
    "print(results)\n",
    "\n",
    "# 10. é¢„æµ‹ä¸åˆ†ç±»æŠ¥å‘Š\n",
    "predictions = trainer.predict(eval_dataset)\n",
    "pred_labels = predictions.predictions.argmax(axis=-1)\n",
    "print(\"åˆ†ç±»æŠ¥å‘Šï¼š\")\n",
    "print(classification_report(eval_dataset[\"label\"], pred_labels))\n",
    "roc_auc = roc_auc_score(eval_dataset[\"label\"], predictions.predictions[:, 1])\n",
    "print(f\"AUC åˆ†æ•°: {roc_auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch_py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
